% Template article for preprint document class `elsart'
% with harvard style bibliographic references
% SP 2006/04/26

%\documentclass{elsart}
\documentclass{elsart5p}

% Use the option doublespacing or reviewcopy to obtain double line spacing
% \documentclass[doublespacing]{elsart}

% the natbib package allows both number and author-year (Harvard)
% style referencing;
\usepackage{natbib}

% if you use PostScript figures in your article
% use the graphics package for simple commands
% \usepackage{graphics}
% or use the graphicx package for more complicated commands
\usepackage{graphicx}
\usepackage{color}
\usepackage{bm}
% or use the epsfig package if you prefer to use the old commands
% \usepackage{epsfig}

% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{listings}
\lstloadlanguages{C}
\lstset{%
 language={C},%
% backgroundcolor={\color[gray]{.85}},%
 basicstyle={\scriptsize\tt},%
identifierstyle={},%
commentstyle={\itshape},%
keywordstyle={\bfseries},%
ndkeywordstyle={},%
stringstyle={\ttfamily},
% identifierstyle={\small},%
%commentstyle={\small\itshape},%
commentstyle={\scriptsize\tt},
% keywordstyle={\small\bfseries},%
% ndkeywordstyle={\small},%
% stringstyle={\small\ttfamily},
 frame={single},%
 breaklines=false,%
 columns=[l]{fixed},%
 numbers=left,%
 tabsize=4,%
% xrightmargin=0zw,%
% xleftmargin=3zw,%
 xleftmargin=10pt,%
 numberstyle={\scriptsize},%
% numberstyle={\small},%
 stepnumber=1,%
% numbersep=1zw,%
% lineskip=-0.5ex,%
 lineskip=0.15ex,%
}
\renewcommand{\lstlistingname}{\small List}


% The lineno packages adds line numbers. Start line numbering with
% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
% for the whole article with \linenumbers.
% \usepackage{lineno}


\newcommand{\itbold}[1]{\textbf{\textit{#1}}}
\newcommand{\pdif}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\odif}[2]{\frac{d #1}{d #2}}
\newcommand{\rotr}[1]{\nabla_{\bf r} \times #1}
\newcommand{\rot}[1]{\nabla \times #1}
\newcommand{\divr}[1]{\nabla_{\bf r} \cdot \textbf{\textit{#1}}}
\renewcommand{\div}[1]{\nabla \cdot #1}

% \linenumbers
\begin{document}

\begin{frontmatter}

% Title, authors and addresses

% use the thanksref command within \title, \author or \address for footnotes;
% use the corauthref command within \author for corresponding author footnotes;
% use the ead command for the email address,
% and the form \ead[url] for the home page:
% \title{Title\thanksref{label1}}
% \thanks[label1]{}
% \author{Name\corauthref{cor1}\thanksref{label2}}
% \ead{email address}
% \ead[url]{home page}
% \thanks[label2]{}
% \corauth[cor1]{}
% \address{Address\thanksref{label3}}
% \thanks[label3]{}

\title{PSDF: Particle Stream Data Format for N-Body Simulations}

% use optional labels to link authors explicitly to addresses:
% \author[label1,label2]{}
% \address[label1]{}
% \address[label2]{}

\author[xx1]{Will Farr\corauthref{cor1}},
\ead{w-farr@northwestern.edu}
\author[xx2]{Jeff Ames},
\author[Princeton]{Piet Hut},
\author[UTokyoTech]{Junichiro Makino},
\author[xx3]{Steve McMillan},
\author[xx4]{Takayuki Muranushi},
\author[xx5]{Koichi Nakamura},
\author[xx6]{Keigo Nitadori}, \&
\author[xx7]{Simon Portegies Zwart}

\address[UTokyoTech]{Interactive Research Center of Science, Graduate
  School of Science and Engineering Tokyo Institute of Technology,
  2--12--1 Ookayama, Meguro, Tokyo 152-8551, Japan}
\corauth[cor1]{Corresponding author.}
\address[Princeton]{Institute for Advanced Study, Princeton, NJ 08540,
  USA}
\address[xx1]{xxx xxx xxx}
\address[xx2]{xxx xxx xxx}
\address[xx3]{xxx xxx xxx}
\address[xx4]{xxx xxx xxx}
\address[xx5]{xxx xxx xxx}
\address[xx6]{xxx xxx xxx}
\address[xx7]{xxx xxx xxx}

\begin{abstract}
  We present a data format for the output of general N-body simulations,
  allowing the presence of individual time steps.  By specifying a standard,
  different N-body integrators and different visualization and analysis
  programs can all share the simulation data, independent of the type of
  programs used to produce the data.  Our Particle Stream Data Format,
  PSDF, is specified in YAML, based on the same approach as XML but
  with a simpler syntax.  Together with a specification of PSDF, we
  provide background and motivation, as well as specific examples in a
  variety of computer languages.  We also offer a web site from which
  these examples can be retrieved, in order to make it easy to augment
  existing codes in order to give them the option to produce PSDF output.
\end{abstract}

\begin{keyword}
% keywords here, in the form: keyword \sep keyword
Stellar dynamics \sep Method: $N$-body simulation

% PACS codes here, in the form: \PACS code \sep code
\end{keyword}

\end{frontmatter}

\section{Introduction}

The simplest N-body calculations use a shared time step length for all
particles.  This implies a rather simple structure of the output.  With
N particles and k time steps, the output takes on the form of an $N*k$
matrix of particle data, where the latter typically contain the mass,
position and velocity of a single particle at a specific time, with
possible additional information such as higher derivatives of the position
(such as acceleration, jerk, etc.), the value of the potential at the 
position of that particle, and so on.  The output of this matrix can
be done by ordering in time or by ordering by the identity of particles,
in which case each world line is output separately.

Some complications may occur when particles are removed, for example
because they are escaping from the system, or because they represent
a star that undergoes a destructive supernova leaving no remnant.
However, the basic I/O structure is simple enough that it is easy to
present these kinds of data in one of the standard data formats, such
as FITS or HDF, with a brief description of what is what.

The situation gets vastly more complicated, though, when we allow for
individual time steps.  Simulations of dense stellar systems, such as
open and globular star clusters, as well as galactic nuclei, have
relied on the use of individual time steps very early on, already in
the 1960s.  The reason is that the presence of close binaries and
triples in such systems would increase the computer power needed by
orders of magnitude in case of shared time steps, compared to
individual time steps.  In addition, cosmological codes, too, are
moving toward the use of individual timesteps, given the increasingly
large discrepancies of intrinsic time scales that come with
increasingly high spatial resolution.

The simplest way to output data from individual time step codes would
be to stick to shared time steps.  Indeed, typical legacy codes, such
as NBODY6, do just that by default.  If all one wants to do is to make
a fixed movie of a simulation run, that approach suffices.  However,
when we interactively inspect the results of a simulation run, we want
to be able to zoom in and out, and speed up and slow down the rate at
which we run the graphics presentation of the run.  With a fixed
initial output rate, it may not be possible to interpolate the motion
of the particles that move at high speeds.  Phrased differently, an
output rate high enough to faithfully present the motion of all
particles may be prohibitively expensive in terms of memory.  It would
be much better to let the graphics program itself decide how and where
to extrapolate, given the original data it has received from a
simulations code.

For example, when we display the dense center of a star cluster,
the graphics program can then use the full information for the rapidly
moving particles, while interpolating the data for the slower halo
particles.  Such an approach can easily save orders of magnitude of
memory storage requirement.  An implementation of this approach was
made by Steve McMillan (199? -- {\bf Steve, please provide reference
and a few-line summary}).  However, this implementation was handcrafted
for a specific code, reflecting the data structure used in that code.
Clearly, it would be desirable to have a more universal data format
that would allow different codes to share data in a more transparent
way.

Other concerns are to make a data format standard machine independent,
to make allowance for parallel processing, and to avoid serious overhead
penalties with respect to performance ({\bf Jun, do you want to add a
few lines here?})

\section{Basic idea}

We need to store the data sufficient to reconstruct the orbits of
individual particles. Conceptually, what we need is a stream of
phase-space information of particles, such as:
\begin{verbatim}
  particle_id, time, mass, x, y, z, vx, vy, vz, ...
  particle_id, time, mass, x, y, z, vx, vy, vz, ...
  particle_id, time, mass, x, y, z, vx, vy, vz, ...
\end{verbatim}
However, the data format must be flexible enough to be able to include
more information such as
\begin{itemize}
  \item radius, and other info related to stellar evolution
  \item merger history
  \item whatever else one can think of
\end{itemize}

One way to construct such a flexible data format is to use
self-describing data format, such as XML or YAML. For simplicity,
we adopt YAML here.

\subsection{Some basics of YAML}

The following is a simple example of a data in YAML format.


\begin{verbatim}
--- !!Particle
id: 0
r:
  - 0.1
  - 0.2
  - 0.3
v:
  - -1
  - -2
  - -3
m: 1.0
\end{verbatim}

In the above example, the line

\begin{verbatim}
--- !!Particle
\end{verbatim}
Is the header, which indicates that it describes the data of an object
of type {\tt Particle}.


\begin{verbatim}
id: 0
\end{verbatim}
defines a field with name ``id'', and value 0.
\begin{verbatim}
r:
  - 0.1
  - 0.2
  - 0.3
\end{verbatim}
means the field ``r'' is an array with three elements. The
first ``-'' means this line is a data for an array.
By default, numbers without ``.'' are regarded as integers, and with
``.'' floating point. Note that indentation has meaning here and ``-''
must be indented the same level or deeper than ``r'' and should be aligned.
Here,  ``v'' and ``m'' are similar.


\section{Particle Stream Data Format}


With the  minimal description of YAML in the previous section, now
we can define the generic data format (not yet for binaries, though):
The data format is the stream of YAML representation of particle
object, like

\begin{verbatim}
--- !!Particle
id: 0
t: 0
r:
  - 0.1
  - 0.2
  - 0.3
v:
  - -1
  - -2
  - -3
m: 1.0
--- !!Particle
id: 1
t: 0
r:
  - 0.2
  - 0.3
  - 0.4
v:
  - 0
  - 0
  - 0
m: 1.0
....
\end{verbatim}

\subsection{Formalities}

For a data format to be understandable by a computer program, there
need to be some convension. We start with the list of reserved words
for the names:

\begin{center}
\begin{tabular}{|c|l|}
\hline
name & description\\
\hline
id & index (can be arbitrary text)\\
m & mass\\
t & time\\
t\_max & max time to which this record is valid \\
r  & position, array with three elements\\
v  & velocity, array with three elements\\
pot  & potential\\
acc  & acceleration, array with three elements\\
jerk  & jerk, array with three elements\\
snap  & snap, array with three elements\\
crackle  & crackle, array with three elements\\
pop  & pop, array with three elements\\
\hline
\end{tabular}
\end{center}

We require that time, position, velocity and higher derivatives are
consistent (that if position is given in parsec and time in year,
velocity must be in parsec/year, not km/s, for example). Also, we
expect the users of this format to follow the convension of using
these names to descrive these physical quantities. Here, {\tt t\_max}
is rather special, in that it gives the maximum possible time that
this record is used to predict the orbit of this particle.

Any of these fields are not required. One can make a record without
velocity, mass, time or whatsoever. If a program needs the positions
of particles but the data file does not give them, the program should
raise error message. We will give example APIs in several languages in
the appendix.

In the a data file, if the record of one particle is not produced
after certain time, the analysis/visualization program regard that it
somehow vanished after time {\tt t\_max}.

We considered the possibility of defining a special record for
creation and destruction of a particle, but decided against that to
keep the parser as simple as possible to write.

We call this format with basic naming convention PSDF (Particle Stream
Data Format). If one wants to add his/her own data, one possibility is
to use a name like ``NBODY6\_predicted\_position''. We do not enforce
the use of keyword  ``X\_'', but are not against it.

We call a YAML document which corresponds to a single particle object
as ``worldpoint'', since it is a point on the worldline associated
with a particle. 


\section{Some remarks}

\subsection{File format}
PSDF does not define the on-memory data description. It does not
define how the individual YAML documents are stored.

There are variety of ways to store worldpoints in file(s). In one
extreme, all worldpoints are stored in a single big file, in the order
defined by the time sequence. In the opposite extreme, one worldpoint is
stored in one file. We could think of anything in between, such as
single file for one particle or single file for some period in time.

In order to do some parallel I/O, using multiple files may be more
natural. We do not yet specify any particular form of parallel access,
though. We might provide some examples, though.


\section{Repository}


\begin{verbatim}
   git@github.com:jmakino/Particle-Stream-Data-Format.git
\end{verbatim}

you can download it by:
\begin{verbatim}
  mkdir foo
  cd foo
  git clone git://github.com/jmakino/Particle-Stream-Data-Format.git
\end{verbatim}

To update:

\begin{verbatim}
  git pull
\end{verbatim}
  
\section{Sample codes}
Also in the github.

\subsection{Base library}
psdf.rb:
\begin{verbatim}
require 'yaml'
class Particle
  def taguri
    return 'x-private:Particle'
  end
end
YAML.add_private_type('Particle') do |type, val|
  YAML.object_maker(Particle, val)
end
\end{verbatim}

This is current minimal ``library'', which defines particle class and
``Particle'' tag.

\subsection{Simple program to generate a psdf file}

writetest.rb:
\begin{verbatim}
require "psdf.rb"
class Particle
  attr_accessor :id, :r   
  def initialize
    @id=0
    @t=0
    @r=[0,1,2]
  end
end
(0..10).each{|id|
  obj=Particle.new
  obj.id =id;
  obj.x[0]=id*0.1
  print YAML.dump(obj)
}
\end{verbatim}

This one generate 10 worldpoints.

\subsection{Simple program to read a psdf file}

readtest.rb:
\begin{verbatim}
require "psdf.rb"
a = []
while s = gets("--- ")
  print s
  print "\n\nend of one gets\n\n"
  print s, "\n"
  obj = YAML.load(s) 
  a.push obj if obj
end
p a
\end{verbatim}

This one reads in 10 particles and store everything in a single array.

\subsection{An ACS Body method to write a psdf worldpoint}

acs\_psdf.rb:
\begin{verbatim}
require "psdf.rb"
class Particle
  attr_accessor :id, :r, :v, :a, :p, :j, :m, :t, :dt
end

class Body
  def to_psdf
    obj=Particle.new
    obj.id = @body_id
    obj.t  = @time
    obj.r = Array[*@pos]
    obj.v = Array[*@vel]
    obj.a = Array[*@acc]
    obj.j = Array[*@jerk]
    obj
  end
  def psdf_output
    print YAML.dump(self.to_psdf)
  end
end
\end{verbatim}

In the ACS Body class, position, velocity and other vector data are of
class {\tt Vector}, not {\tt Array}. So you need to convert them to
{\tt Array}.

\subsection{An OpenGL animation program}

The file testplot.rb gives a simple example. It it intended to be a
skeleton code, with minimal UI and other stuff. It takes an input file
with name ``testin'', in which a sequence of psdf world points is
stored (can come from shared-timestep or individual-timestep code).

This program requires ruby-opengl library. To install this (if not
already installed) try:
\begin{verbatim}
  sudo gem install opengl
\end{verbatim}
On my machine with CentOS 5.5, it failed with:
\begin{verbatim}
sudo gem install ruby-opengl
Building native extensions.  This could take a while...
ERROR:  Error installing ruby-opengl:
        ERROR: Failed to build gem native extension.

/usr/bin/ruby -rubygems /usr/lib/ruby/gems/1.8/gems/rake-0.8.7/bin/rake =
RUBYARCHDIR=3D/usr/lib/ruby/gems/1.8/gems/ruby-opengl-0.60.1/lib =
RUBYLIBDIR=3D/usr/lib/ruby/gems/1.8/gems/ruby-opengl-0.60.1/lib
/usr/bin/ruby mkrf_conf.rb
(in /usr/lib/ruby/gems/1.8/gems/ruby-opengl-0.60.1)
rake
gcc  -fPIC -g -O2    -Wall -DRUBY_VERSION=3D187  -I/usr/include =
-I/usr/lib/ruby/1.8/x86_64-linux -I/usr/lib/ruby/site_ruby/1.8 -I. -c =
gl-enums.c
(in /usr/lib/ruby/gems/1.8/gems/ruby-opengl-0.60.1/ext/gl)
gl-enums.c:6  include :
../common/common.h:45:21: error: GL/glut.h:=20
                                              rake aborted!
Command failed with status (1): [gcc  -fPIC -g -O2    -Wall =
-DRUBY_VERSION=3D...]
/usr/lib/ruby/gems/1.8/gems/ruby-opengl-0.60.1/ext/gl/Rakefile:29
(See full trace by running task with --trace)
rake aborted!
Command failed with status (1): [rake...]

(See full trace by running task with --trace)


Gem files will remain installed in =
/usr/lib/ruby/gems/1.8/gems/ruby-opengl-0.60.1 for inspection.
Results logged to =
/usr/lib/ruby/gems/1.8/gems/ruby-opengl-0.60.1/gem_make.out
\end{verbatim}

In this case, you need to install GLUT library, by
\begin{verbatim}
  sudo yum install freeglut   freeglut-devel
\end{verbatim}

\section*{Acknowledgment}

\begin{thebibliography}{00}
  \bibitem[Tanikawa et al.(2011)]{Tanikawa11} Tanikawa,~A., Hut,~P.,
    \& Makino,~J. 2011, arXiv:astro-ph/1107.3866
\end{thebibliography}

\end{document}
